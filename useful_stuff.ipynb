{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "# Some useful stuff\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dms_total = dms_total.repartition(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_distr = sales.filter('weekStart like \"%2016%\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scp jupyterhub:/mapr/prdfcmaprwithspark1/production/markets/vietnam/datascience/extra_columns .\n",
    "# scp file_to_upload jupyterhub:~     -- upload to your one home directory on jupyterhub\n",
    "# scp file_to_upload jupyterhub:/mapr/prdfcmaprwithspark1/production/markets/vietnam/datascience/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to timestamp\n",
    "from pyspark.sql import Column\n",
    "\n",
    "def to_timestamp(\n",
    "       column_name: str,\n",
    "       dest_col_name: str,\n",
    "       ts_format: str = \"yyyy-MM-dd HH:mm:ss\") -> Column:\n",
    "    return (unix_timestamp(col(column_name), ts_format)\n",
    "           .cast(TimestampType())\n",
    "           .alias(dest_col_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# weeks\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "def pandas_weeks_table(start_date, end_date=None):\n",
    "    \"\"\"\n",
    "        Creates a Pandas dataframe that represents a week table\n",
    "        containing the date when a week starts\n",
    "        respectively the date when a weeks ends,\n",
    "        starting the table from start_date and ending the table at end_date.\n",
    "\n",
    "        :param start_date:\n",
    "            Represents the start of the weeks table.\n",
    "            If the start date is in the middle of a week (not a Monday)\n",
    "            the table will be started from the most recent Monday.\n",
    "\n",
    "        :param end_date:\n",
    "            Represents the end of the weeks table.\n",
    "            If the end date is in the middle of a week (not a Sunday) the\n",
    "            table will be started from the nest Sunday.\n",
    "            It the end_date is not specified then today will be used instead.\n",
    "\n",
    "        >>> pandas_weeks_table(start_date='2018-01-02', end_date='2018-02-01')\n",
    "           week_start    week_end\n",
    "        0  2018-01-01  2018-01-07\n",
    "        1  2018-01-08  2018-01-14\n",
    "        2  2018-01-15  2018-01-21\n",
    "        3  2018-01-22  2018-01-28\n",
    "        4  2018-01-29  2018-02-04\n",
    "\n",
    "        >>> pandas_weeks_table(\n",
    "        ...    start_date='2018-01-02 23:34:59',\n",
    "        ...    end_date='2018-02-01 21:31:51')\n",
    "           week_start    week_end\n",
    "        0  2018-01-01  2018-01-07\n",
    "        1  2018-01-08  2018-01-14\n",
    "        2  2018-01-15  2018-01-21\n",
    "        3  2018-01-22  2018-01-28\n",
    "        4  2018-01-29  2018-02-04\n",
    "    \"\"\"\n",
    "    end_table = (\n",
    "        (end_date and pd.Timestamp(end_date))\n",
    "        or pd.Timestamp.today())\n",
    "    start_table = pd.Timestamp(start_date).normalize()\n",
    "    week_start = (\n",
    "        pd.date_range(\n",
    "            start=start_table - pd.Timedelta(6, unit='D'),\n",
    "            end=end_table,\n",
    "            freq='W-MON')\n",
    "        .to_series()\n",
    "        .dt.date\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    week_end = (\n",
    "        pd.date_range(\n",
    "            start=start_table,\n",
    "            end=end_table + pd.Timedelta(6, unit='D'),\n",
    "            freq='W-SUN')\n",
    "        .to_series()\n",
    "        .dt.date\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    table = pd.DataFrame(\n",
    "        [week_start, week_end],\n",
    "        ['week_start', 'week_end']\n",
    "    ).transpose()\n",
    "\n",
    "    return table\n",
    "\n",
    "\n",
    "def spark_weeks_table(start_date, end_date=None):\n",
    "    \"\"\"\n",
    "        Creates a Spark DataFrame that represents a week table\n",
    "        containing the date when a week starts respectively\n",
    "        the date when a weeks ends, plus the week number in the year\n",
    "        and the year itself.\n",
    "        The first week will be the week where start_date is,\n",
    "        and the last week will be the week where end_date is.\n",
    "\n",
    "        :param spark:\n",
    "            The Spark session.\n",
    "\n",
    "        :param start_date:\n",
    "            Represents the start of the weeks table.\n",
    "            If the start date is in the middle of a week (not a Monday)\n",
    "            the table will be started from the most recent Monday.\n",
    "\n",
    "        :param end_date:\n",
    "            Represents the end of the weeks table.\n",
    "            If the end date is in the middle of a week (not a Sunday)\n",
    "            the table will be started from the nest Sunday.\n",
    "            It the end_date is not specified then today will be used instead.\n",
    "\n",
    "        >>> weeks_table = spark_weeks_table(\n",
    "        ...    start_date='2015-12-30 23:34:59',\n",
    "        ...    end_date='2016-01-12 21:31:51')\n",
    "\n",
    "        >>> weeks_table.printSchema()\n",
    "        root\n",
    "         |-- week_start: date (nullable = true)\n",
    "         |-- week_end: date (nullable = true)\n",
    "         |-- week_number: integer (nullable = true)\n",
    "         |-- year: integer (nullable = true)\n",
    "        <BLANKLINE>\n",
    "\n",
    "        >>> weeks_table.collect()  # doctest: +NORMALIZE_WHITESPACE\n",
    "        [Row(week_start=datetime.date(2015, 12, 28),\n",
    "             week_end=datetime.date(2016, 1, 3),\n",
    "             week_number=53,\n",
    "             year=2016),\n",
    "         Row(week_start=datetime.date(2016, 1, 4),\n",
    "             week_end=datetime.date(2016, 1, 10),\n",
    "             week_number=1,\n",
    "             year=2016),\n",
    "         Row(week_start=datetime.date(2016, 1, 11),\n",
    "             week_end=datetime.date(2016, 1, 17),\n",
    "             week_number=2,\n",
    "             year=2016)]\n",
    "    \"\"\"\n",
    "    # since there should be a single spark session\n",
    "    # this is safe to do and it will return the active session\n",
    "    # useful since otherwise I would have needed to have spark as parameter\n",
    "    # when the doctest are run, a spark session will be create on local\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    week_table = pandas_weeks_table(start_date, end_date)\n",
    "\n",
    "    weeks = (\n",
    "        spark\n",
    "        .createDataFrame(week_table)\n",
    "        .withColumn('week_number', F.weekofyear('week_end'))\n",
    "        .withColumn('year', F.year('week_end')))\n",
    "    return weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "friso_top_prov = friso_mapped_all.where(friso_mapped_all.province.isin(top_provinces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "days = lambda i: i * 86400 \n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "w = (Window()\n",
    "   .partitionBy(col(\"account_id\"))\n",
    "   .orderBy(col(\"activity_date_a\").cast(\"timestamp\").cast(\"long\"))\n",
    "   .rangeBetween(-days(7), 0))\n",
    "\n",
    "df = (df.select(col(\"*\"), sum(\"n_days_last_activity\").over(w).alias(\"sum_active_days\"),\n",
    "          F.min('activity_date_a').over(w).alias('min_date'),\n",
    "         F.max('activity_date_a').over(w).alias('max_date')))\n",
    "\n",
    "\n",
    "df = df.withColumn('days_diff',datediff(to_date(unix_timestamp('max_date','yyyy-MM-dd').cast('timestamp')),\\\n",
    "                                        to_date(unix_timestamp('min_date','yyyy-MM-dd').cast('timestamp'))))\n",
    "\n",
    "\n",
    "df = df.withColumn('avg_days_inactivity',col('days_diff')/col('sum_active_days'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create table in which every stage column is divided by its marco polo usage mapping\n",
    "\n",
    "sales_mp = sales_stages1\n",
    "#divide every stage column\n",
    "for col_stage, kg_mp, stage in zip(['StagePregnant_totalKG','Stage1_totalKG','Stage2_totalKG','Stage3_totalKG',\\\n",
    "                              'Stage4_totalKG','Stage5_totalKG'],\\\n",
    "                             [2.8, 2.9, 4.3, 6, 4.6, 3.4],\\\n",
    "                                   ['StagePregnant', 'Stage1', 'Stage2', 'Stage3', 'Stage4', 'Stage5']):\n",
    "    sales_mp = sales_mp.withColumn('{}_mums'.format(stage), sales_mp[col_stage] / int(kg_mp))\n",
    "  #replace nans with 0 in number of mums\n",
    "sales_mp = sales_mp.fillna(0, subset=['StagePregnant_mums','Stage1_mums','Stage2_mums','Stage3_mums',\\\n",
    "                   'Stage4_mums','Stage5_mums'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pivot by stage\n",
    "sales_stages1 = sales_stages.groupby('weekStart', 'weekEnd', 'district')\\\n",
    "                            .pivot('Type').sum('summedProductsKG')\n",
    "\n",
    "#rename columns\n",
    "for col in ['Stage1', 'Stage2', 'Stage3', 'Stage4', 'Stage5']:\n",
    "    sales_stages1 = sales_stages1.withColumnRenamed(col, col+'_totalKG')\n",
    "for col in ['Pregnant', 'Special']:\n",
    "    sales_stages1 = sales_stages1.withColumnRenamed(col, 'Stage'+col+'_totalKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#first we need to have the date formatted without time, otherwise in the same day dates are read as different\n",
    "act_df = act_df.withColumn('act_day', to_date(col('act_created')))\n",
    "#group to calculate number of activities per day, per day and account\n",
    "daily_group = act_df.groupby('accountId', 'act_day').agg(count(col('act_created')).alias('n_activities'))\n",
    "#\n",
    "today_df = spark.createDataFrame([[today,0]], ['act_day', 'n_activities']) #df with today's date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_check = accounts_delta.withColumn('CreatedOn_X',  from_unixtime(unix_timestamp('CreatedOn','MM/dd/yyy hh:mm:ss')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for each account, calc time difference with next row\n",
    "\n",
    "df = act_today_df\n",
    "my_window = Window.partitionBy(\"accountId\").orderBy(\"act_day\")     #create a window in the dataframe\n",
    "\n",
    "#add column with previous date\n",
    "df = df.withColumn(\"prev_date\", F.lag(df.act_day).over(my_window))\n",
    "#difference between every date and the previous one\n",
    "df = df.withColumn('date_diff', datediff(col('act_day'), col('prev_date')))\n",
    "df = df.repartition(50).persist(StorageLevel.DISK_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/media/sf_shared_folder\n",
    "scp  scp Income-postcode-2004-2014.csv ec2-user@fc1:/home/ec2-user/\n",
    "and vice versa\n",
    "scp ec2-user@fc1:/home/ec2-user/outputNielsen.html outputNielsen.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inner, outer etc joining\n",
    "# https://pandas.pydata.org/pandas-docs/stable/merging.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#First format the CREATE_DT to the right format\n",
    "start_table = start_table.withColumn(\"formatDT\",from_unixtime(unix_timestamp('CREATE_DT', 'MM/dd/yyyy hh:mm:ss')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rename_columns(df,dict_cols):\n",
    "    for k,v in dict_cols.items():\n",
    "        df = df.withColumnRenamed(k,v)\n",
    "    return df\n",
    "\n",
    "dict_cols = {\"thong_tin_nielsen_cap_nhat_thuc_te__PHUONG/_XA\":\"ward\",\\\n",
    "            \"thong_tin_nielsen_cap_nhat_thuc_te__QUAN/_HUYEN\":\"district\",\\\n",
    "            \"thong_tin_nielsen_cap_nhat_thuc_te__THANH_PHO/TINH\":\"province\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def columns_renamed(df,dict_cols):\n",
    "    for k, v in dict_cols.items():\n",
    "        df = df.withColumnRenamed(k,v)\n",
    "    return df\n",
    "\n",
    "\n",
    "dict_cols = {\"ADDR_1\":\"address\",\"ADDR_3\":\"ward\",\"ADDR_4\":\"district\",\"ADDR_5\":\"province\"}\n",
    "cust_address = cust_v5.select(list(dict_cols.keys()) + [\"CUST_CD\",\"DIST_CD\",\"ADDR_2\",\"ADDR_6\"])   \n",
    "#df with customer and adresses\n",
    "cust_address = columns_renamed(cust_address,dict_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now only get the unique districts among the not mapped ones\n",
    "unique_dist = cust_address.select(\"district\").distinct()\n",
    "#print(unique_dist.count())\n",
    "\n",
    "def remove_accents(input_str):\n",
    "    #Removing the accents of the string\n",
    "    return unidecode.unidecode(input_str)\n",
    "\n",
    "func_udf = udf(lambda x: remove_accents(x),StringType())\n",
    "\n",
    "#List of columns that needs to be upper case and removing of the accents\n",
    "list_cols = [\"district\"]\n",
    "for column in list_cols:\n",
    "    unique_dist = unique_dist.withColumn(\"NoAccent_{}\".format(column),func_udf(upper(col(column))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_mapping = dms_mapping.collect()     \n",
    "#make a copy of dms_mapping containing the start column and the formatted version\n",
    "\n",
    "to_replace = []\n",
    "value_replace = []\n",
    "for row in list_mapping:\n",
    "    #Add the start of the mapping\n",
    "    to_replace.append(row[\"start\"])\n",
    "    #Add the format results so we can later use it with the regexp replace\n",
    "    value_replace.append(row[\"format_result\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#added formatted version to the unique not mapped districts\n",
    "#Make a new column with the format district\n",
    "unique_dist = unique_dist.withColumn(\"format_district\",col(\"NoAccent_district\"))\n",
    "#Replace the start from the above cell with the value of the format_result\n",
    "unique_dist = unique_dist.replace(to_replace,value_replace,subset=[\"format_district\"])\n",
    "if verbose:\n",
    "    unique_dist.show()\n",
    "    unique_dist.groupby(\"format_district\").count().sort(\"count\", ascending = False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ADD COLUMN WITH TOTAL AVERAGE PER YEAR\n",
    "\n",
    "def sum_(*cols):                        #define function so I can use a list of column names\n",
    "      return reduce(add, cols, lit(0))\n",
    "\n",
    "brand_activ = brand_activ.withColumn('total_avgyears', sum_(*[col(x) for x in brand_avg_cols] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build a dataframe containing the accountid, and percentages of the first,\n",
    "#second and third favorites (supercompact command that Emanuel made out of magic)\n",
    "\n",
    "brand = (brand_act\n",
    " .withColumn('sorted_values', F.sort_array(F.array(*brand_perc_cols), asc=False))    #adds a column containing a list with\n",
    "                                                                                    #the sorted values of the perc columns\n",
    " .select(                                                                           #makes new dataframe containing accountid\n",
    "     'accountid',\n",
    "     F.col('sorted_values').getItem(0).alias('max1'),                               #and the three max percentages\n",
    "     F.col('sorted_values').getItem(1).alias('max2'),\n",
    "     F.col('sorted_values').getItem(2).alias('max3')))\n",
    "\n",
    "\n",
    "\n",
    "brand_nofil = (brand_act_nofil\n",
    " .withColumn('sorted_values', F.sort_array(F.array(*brand_perc_cols), asc=False))    #adds a column containing a list with\n",
    "                                                                                    #the sorted values of the perc columns\n",
    " .select(                                                                           #makes new dataframe containing accountid\n",
    "     'accountid',\n",
    "     F.col('sorted_values').getItem(0).alias('max1'),                               #and the three max percentages\n",
    "     F.col('sorted_values').getItem(1).alias('max2'),\n",
    "     F.col('sorted_values').getItem(2).alias('max3')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Length of the string in a udf function\n",
    "func = udf(lambda xs: len(xs), IntegerType())\n",
    "\n",
    "#The brand with each regex expression in it\n",
    "brand_regex = {\"Friso\":'[f|F]riso|FRISO',\n",
    "               \"CPL\":\"CPL\",\n",
    "               \"DutchLady\":\"DL[^a-zA-Z]|CGHL|DUTCH LADY|DUTCH BABY\",\n",
    "               \"Fristi\":\"[f|F]risti|FRISTI\",\n",
    "              #\"Nubei\":\"\", Could not find any information about this brand\n",
    "              \"Ovaltine\":\"[o|O]valtine|OVALTINE\",\n",
    "              \"POSM\":\"POSM\",\n",
    "              \"TS\":\"TS\",\n",
    "              \"Yomost\":\"[Y|y]omost|YOMOST|YM\"}\n",
    "#Get the brand names. This is just the keys of the regex brands\n",
    "brand_names = brand_regex.keys()\n",
    "\n",
    "\n",
    "#Parse from the PRD_DESC,PRD_DESC1,PRD_DESC2, the regex from each brand and check \n",
    "#for all product description which is the regex. Then pick the greatest of the three product descriptions\n",
    "def parse_regex(df,name,regex):\n",
    "    df = df.withColumn(\"{}_desc0\".format(name),func(regexp_extract(col(\"PRD_DESC\"),regex,0)))\n",
    "    df = df.withColumn(\"{}_desc1\".format(name),func(regexp_extract(col(\"PRD_DESC1\"),regex,0)))\n",
    "    df = df.withColumn(\"{}_desc2\".format(name),func(regexp_extract(col(\"PRD_DESC2\"),regex,0)))\n",
    "    df = df.withColumn(\"{}_all\".format(name),greatest(\"{}_desc0\".format(name),\"{}_desc1\".format(name),\\\n",
    "                                                      \"{}_desc2\".format(name)))\\\n",
    "           .drop(\"{}_desc0\".format(name)).drop(\"{}_desc1\".format(name)).drop(\"{}_desc2\".format(name))\n",
    "    return df\n",
    "\n",
    "#Do it for all the brands\n",
    "for name in brand_names:\n",
    "    prd_CD = parse_regex(prd_CD,name,brand_regex[name])\n",
    "\n",
    "#Parse the brand from the above maded columns with all. If this is bigger then 0, then it should have a regex that is\n",
    "#bigger then 0, so it should have that brand at that point else it will return unknown\n",
    "def parse_brand_col(df):\n",
    "    if df.Yomost_all > 0:\n",
    "        return \"Yomost\"\n",
    "    elif df.Fristi_all > 0:\n",
    "        return \"Fristi\"\n",
    "    elif df.Ovaltine_all > 0:\n",
    "        return \"Ovaltine\"\n",
    "    elif df.DutchLady_all > 0:\n",
    "        return \"DutchLady\"\n",
    "    elif df.POSM_all > 0:\n",
    "        return \"POSM\"\n",
    "    elif df.Friso_all > 0:\n",
    "        return \"Friso\"\n",
    "    elif df.TS_all > 0:\n",
    "        return \"TS\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "    \n",
    "parse_brand = udf(parse_brand_col,StringType())\n",
    "\n",
    "prd_CD = prd_CD.withColumn(\"Brand\", parse_brand(struct(*prd_CD.columns)))\n",
    "#Clean up the dataframe.\n",
    "for name in brand_names:\n",
    "    prd_CD = prd_CD.drop(\"{}_all\".format(name))\n",
    "\n",
    "prd_CD.show()\n",
    "prd_CD.printSchema()\n",
    "prd_CD.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "not_friso = prd_CD\n",
    "print(not_friso.count())\n",
    "\n",
    "#Parse the fully product size. This should be something like 30X400gr or 10X250ML\n",
    "not_friso = not_friso.withColumn(\"product_size_full\",regexp_extract(col(\"PRD_DESC1\"),'((\\d+[X|x])?(\\d+.\\d+))(ML|ml|g|K?G)r?', 1))\n",
    "#Parse the product size. This should be 400 or 250\n",
    "not_friso = not_friso.withColumn(\"product_size\",regexp_extract(col(\"product_size_full\"),'[X|x](\\d+|\\d+.\\d+)', 1))\n",
    "#This should be gr or ML\n",
    "not_friso = not_friso.withColumn(\"product_dimension\",regexp_extract(col(\"PRD_DESC1\"),'((\\d+[X|x])?(\\d+.\\d+))(ML|ml|g|K?G)r?', 4))\n",
    "#Show everything\n",
    "not_friso.select(\"PRD_DESC1\",\"product_size_full\",\"product_size\",\"product_dimension\").show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PRD_DESC1                        |product_size_full|product_size|product_dimension|\n",
    "+---------------------------------+-----------------+------------+-----------------+\n",
    "|UHT DL STRAWBERRY 48X110ml       |48X110           |110         |ml               |\n",
    "|UHT FRISTI CHOCOLATE 48X110ml    |48X110           |110         |ml               |\n",
    "|UHT FRISTI STRAWBERRY 48X110ml   |48X110           |110         |ml               |\n",
    "|DKY YOMOST ORANGE CB 48X180ml    |48X180           |180         |ml               |\n",
    "|DKY YOMOST STRAWBERRY CB 48X180ml|48X180           |180         |ml               |\n",
    "|IMP DL VANILLA 12X900g           |12X900           |900         |g                |\n",
    "|IMP DL VANILLA 24X400g           |24X400           |400         |g                |\n",
    "|IMP DL VANILLA BIB 24X400g       |24X400           |400         |g                |\n",
    "|IMP DL VANILLA BIB 24X400g       |24X400           |400         |g                |\n",
    "|IMP DL123 VANILLA 12X900g PRO    |12X900           |900         |g                |\n",
    "+---------------------------------+-----------------+------------+-----------------+\n",
    "only showing top 10 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove the CUST_CD and DIST_CD that are already in the nielson_data. Because we can already map them with the \n",
    "#district of the nielson file\n",
    "#Get the nielson keys\n",
    "dms_nielson_keys = dms_nielson.select(\"CUST_CD\",\"DIST_CD\").dropDuplicates()\n",
    "#Get the DMS keys\n",
    "cust_v5_keys = cust_v5.select(\"CUST_CD\",\"DIST_CD\").dropDuplicates()\n",
    "#Get the difference\n",
    "difference_keys = cust_v5_keys.subtract(dms_nielson_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Profiling:\n",
    "import spark_df_profiling\n",
    "    \n",
    "    \n",
    "    Nielson_data.cache()\n",
    "profile = spark_df_profiling.ProfileReport(df = Nielson_data,spark = spark)\n",
    "\n",
    "profile.to_file(outputfile = \"/mapr/fcmaprwithspark1/test_zone/ds_playground/outputNielsen.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- vim editor: \n",
    "to save and quit type :wq enter\n",
    "to insert press Insert button\n",
    "to exit Insert mode press Esc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#extract the brand from mailing_name, using regex\n",
    "\n",
    "products_regex = {'wellness':'^.*([S|s]auna|[W|w]ellness|[T|t]hermen|[S|s]pa|[B|b]eauty|[H|h]ealth|[Z|z]ontegoed|[E|e]lysium|[O|o]ntspanningspakket).*$',\n",
    "               'kadeaubon':'^.*([K|k]adobon|[V|v]oucher|[C|c]adeaubon|[W|w|aardenbon]).*$',\n",
    "                  'clothes':'^.*([Z|z]alando|[S|s]choenen|[A|a]didas).*$'\n",
    "                  'food':'^.*([P|p]annenkoek|[G|g]ourmet|[T|t]hee|[C|c]hocolade).*$',\n",
    "               'film':'^.*([F|f]ilm|[P|p]athe).*$',\n",
    "                  'theater': '^.*([S|s]ister act|[W|w]icked|[P|p]oppins|[S|s]aigon|[M|m]usical|[R|r]ang|[W|w]e will wock you).*$',\n",
    "                  'outings': '^.*([Z|z]oo|[A|a]dventure|[A|a]vonturen|[W|w]aarbeek|[E|e]ntree|[W|w]ildlands|[A|a]penheul|[D|d]ierenpark|[P|p]ark|[D|d]elfinarium|[D|d]rievliet|[S|s]ea life|Bobbejaanland|het loo|[C|c]orpus|[K|k]abouterland|[E|e]comare|[P|p]lopsa indoor|[A|a]viodrome|[M|madame [T|t]ussauds|[F|f]estival|[S|s]peelland|[B|b]allorig).*$', \n",
    "                  'sport':'^.*([S|s]port|[V|v]oetbal|[M|m]arikenloop|[F|f]itness|[Z|z]wem|[R|r]unner).*$',\n",
    "               'magazine':'^.*([M|m]agazine|[W|w]eekblad).*$',\n",
    "                 'hotel_holidays':'^.*([H|h]otel|[S|s]eaways|[B|b]elvilla|[V|v]errassingsreis|[B|b]ed & [B|b]reakfast|[W|w]eertebergen|[V|v]akantiewoning).*$',\n",
    "                 'house': '^.*([B|b]adgoed|[M|m]elkopschuimer|[P|p]hilips|[P|p]avina|[R|r]ituals|[D|d]ekbed|[T|t]as|[F|f]atboy|[C|c]up).*$'}\n",
    "\n",
    "#add a new column called mailing_brand, containing the name of the regex\n",
    "df = EU_orderProduct.select('name', 'artcode').withColumn('products_cat', col('name'))\n",
    "\n",
    "for name, regex in products_regex.items():\n",
    "    df = df.withColumn('products_cat', regexp_replace('products_cat', str(regex), str(name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = emails_part\n",
    "\n",
    "\n",
    "df.agg(F.min(df['email_sent_date'])).show()\n",
    "\n",
    "emails_EU.filter(\"E_sendtriggermail1date != '0000-00-00'\").select(F.min(emails_EU['E_sendtriggermail1date'])).show()\n",
    "emails_EU.filter(\"E_sendtriggermail2date != '0000-00-00'\").select(F.min(emails_EU['E_sendtriggermail2date'])).show()\n",
    "emails_EU.filter(\"E_sendtriggermail3date != '0000-00-00'\").select(F.min(emails_EU['E_sendtriggermail3date'])).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "df = emails_EU\n",
    "\n",
    "\n",
    "df = df.withColumn(\"delta_days\", F.datediff(df.modified, df.E_email_sent_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def replace_type(column, value):\n",
    "    return (when (column == value, lit(1)).otherwise(lit(0)))\n",
    "\n",
    "def sent_date(x, y):\n",
    "    return (when(x == \"Sent\", y).otherwise(\"null\"))\n",
    "    \n",
    "df = emails_part \n",
    "\n",
    "df = df.withColumn('sent', replace_type(F.col('DMDtype'), 'Sent'))\n",
    "df = df.withColumn('open', replace_type(F.col('DMDtype'), 'Open'))\n",
    "df = df.withColumn('trigger', replace_type(F.col('DMDtype'), 'Trigger'))\n",
    "df = df.withColumn('click', replace_type(F.col('DMDtype'), 'Click'))\n",
    "df = df.withColumn('unsubscribe', replace_type(F.col('DMDtype'), 'Unsubscribe'))\n",
    "df = df.withColumn('softbounce 1x', replace_type(F.col('DMDtype'), 'SoftBounce 1x'))\n",
    "df = df.withColumn('hardbounce final', replace_type(F.col('DMDtype'), 'HardBounce final'))\n",
    "df = df.withColumn('spamcomplaint', replace_type(F.col('DMDtype'), 'Spamcomplaint'))\n",
    "df = df.withColumn('sent_date', sent_date(F.col(\"DMDtype\"), F.col(\"DMDlogDate\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.groupBy(\"accountid\").agg({'sent': 'sum','open':'sum','trigger': 'sum', 'click':'sum','unsubscribe':'sum','softbounce 1x':'sum', 'hardbounce final':'sum', 'spamcomplaint':'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import * \n",
    "import pyspark.sql.functions as func \n",
    "\n",
    "def email_sent(t, d):\n",
    "    if t == 'Sent':\n",
    "        return d\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "email_sent_udf = func.udf(email_sent, StringType())\n",
    "\n",
    "df_r = df_r.withColumn('email_sent_date', email_sent_udf('DMDtype', 'DMDlogDate'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1_tmp = df1.groupBy(\"E_accountId\").agg({\"delta_days_pos\":\"sum\", \"delta_days_neg\": \"sum\",\"delta_days_null\": \"sum\"}).\\\n",
    "withColumnRenamed(\"sum(delta_days_pos)\", \"sum_delta_days_pos\").\\\n",
    "withColumnRenamed(\"sum(delta_days_neg)\", \"sum_delta_days_neg\").\\\n",
    "withColumnRenamed(\"sum(delta_days_null)\", \"sum_delta_days_null\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import * \n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "def int_to_string(x):\n",
    "    return str(x)\n",
    "\n",
    "df = df_cnt\n",
    "\n",
    "int_to_stringUDF = F.udf(int_to_string, StringType())\n",
    "\n",
    "df = df.withColumn(\"delta_days_S\", int_to_stringUDF(\"delta_days\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Number of rows: \",emails_EU.count())\n",
    "\n",
    "print(\"Number of distinct accounts: \", emails_EU.select(\"E_accountid\").distinct().count())\n",
    "\n",
    "print(\"Min date in emails: \", emails_EU.agg(F.min(F.col(\"E_sent_date\"))).show())\n",
    "print(\"Max date in emails: \", emails_EU.agg(F.max(F.col(\"E_sent_date\"))).show())\n",
    "count().sort(desc('count')).show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "calls_types={'27':'RawData', '29':'RawData', '23':'Recruitment', '24':'Recruitment', '25':'Retention', '26':'Retention', '28':'CheckNewUser', '30':'CheckNewUser', '6':'Hotline', '1':'UpdateProfile', '3':'DropOut', '':'Other'}\n",
    "\n",
    "tmp = phone_df.filter(\"StateCode!='0'\") \\\n",
    "        .replace(to_replace=calls_types, value=[], subset=['new_CampaignType']) \\\n",
    "        .withColumn(colName='new_CampaignType', col=regexp_replace('new_CampaignType', '[0-9]+', 'Other')) \\\n",
    "        .withColumn(colName='Date', col=regexp_replace('ActualEnd', '-[0-9]+-[0-9]+ [0-9]+:[0-9]+:[0-9]+', '')) \\\n",
    "        .groupBy('Date', 'new_CampaignType') \\\n",
    "        .count() \\\n",
    "        .groupBy('Date') \\\n",
    "        .pivot(pivot_col='new_CampaignType', values=list(set(val for val in calls_types.values()))) \\\n",
    "        .sum('count') \\\n",
    "        .dropna(subset=['Date']) \\\n",
    "        .fillna(0) \\\n",
    "        .orderBy('Date')\n",
    "        \n",
    "tmp.coalesce(1).write.csv('/mapr/fcmaprwithspark1/test_zone/ds_playground/projects/marketing_conversion/2.5_analysis/results/call_types_ts-year.csv', header='true', mode='overwrite')\n",
    "print('Saved successfully!')\n",
    "tmp.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_counts_group(df, column):\n",
    "    return df.groupBy(column).count().sort(\"count\", ascending = False).show(10,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cumulative Sum\n",
    "\n",
    "features_tab.registerTempTable(\"tmp\")\n",
    "\n",
    "df2 = sqlContext.sql(\"SELECT tmp.*, \\\n",
    "    sum(CombinedLabel) OVER (PARTITION BY Contact_Id ORDER BY Green_ActualEnd) as cumsum FROM tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# From pandas to spark data frame\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# Random Forest\n",
    "predictions = pd.DataFrame(predictions)\n",
    "sqlCtx = SQLContext(sc)\n",
    "\n",
    "predictions = sqlCtx.createDataFrame(predictions)\n",
    "\n",
    "predictions.write.parquet(\"/mapr/fcmaprwithspark1/test_zone/ds_playground/projects/marketing_conversion/4_modelling/data/scikit_random_forest\", mode=\"overwrite\")\n",
    "print('Saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write as a data frame \n",
    "\n",
    "\n",
    "labels=test[[label_]].as_matrix()\n",
    "\n",
    "res_df = sqlContext.createDataFrame(\\\n",
    "                                pd.DataFrame(\\\n",
    "                                            np.concatenate(\\\n",
    "                                                           [predictions.reshape(predictions.shape[0], 1), probabilities, labels],\\\n",
    "                                                           axis=1\\\n",
    "                                                           ), \\\n",
    "                                            columns=['Prediction','Probability_0', 'Probability_1','Label']\\\n",
    "                                            ),\\\n",
    "                                samplingRatio=0.1\\\n",
    "                                   )\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importance of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def importance_features(model, X):\n",
    "    importances = model.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in model.estimators_],\n",
    "             axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "    print(\"Feature ranking:\")\n",
    "\n",
    "    for f in range(X.shape[1]):\n",
    "        print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "    plt.figure()\n",
    "    plt.title(\"Feature importances\")\n",
    "    plt.bar(range(X.shape[1]), importances[indices],\n",
    "                       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "    plt.xticks(range(X.shape[1]), indices)\n",
    "    plt.xlim([-1, X.shape[1]])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importance_features(model = rf, X = trainArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dummy dataframe\n",
    "\n",
    "data = sc.parallelize([ \n",
    "[('PackSize', 1.0), ('Name', 'A')],\n",
    "[('PackSize', 1.0), ('Name', 'B')],\n",
    "[('PackSize', 30.0), ('Name', 'C')]\n",
    "    ])\n",
    "# Convert to tuple\n",
    "data_converted = data.map(lambda x: (x[0][1], x[1][1]))\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"Packsize\", DoubleType(), True),\n",
    "    StructField(\"Name\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create dataframe\n",
    "DF = sqlContext.createDataFrame(data_converted, schema)\n",
    "DF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate cumulative sum\n",
    "\n",
    "DF.registerTempTable('df')\n",
    "df2 = sqlContext.sql(\" SELECT sum(Packsize) OVER (ORDER BY Name) as cumsum FROM df\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How to unpack vector\n",
    "\n",
    "results_df = spark.read.parquet(\"/mapr/fcmaprwithspark1/test_zone/ds_playground/projects/marketing_conversion/4_modelling/data/result_logisticreg\")\n",
    "results_df.select('probability').show(10, False)\n",
    "results_df.groupBy('probability').count().orderBy(desc('count')).show(20, False)\n",
    "\n",
    "#\n",
    "\n",
    "tmp = results_df.rdd.map(lambda x: (x[4], float(x[3][0]), float(x[3][1]) ) ).toDF(['prediction', 'Prob0', 'Prob1'])\n",
    "tmp.show(100)\n",
    "tmp.groupBy('Prob1').count().orderBy(desc('count')).show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract cumulative gain\n",
    "\n",
    "# Load results\n",
    "results_df = spark.read.parquet(\"/mapr/fcmaprwithspark1/test_zone/ds_playground/projects/marketing_conversion/4_modelling/data/result_logisticreg\")\n",
    "results_df.groupBy('probability').count().orderBy(desc('count')).show(5, False)\n",
    "results_df.printSchema()\n",
    "results_df.show()\n",
    "#\n",
    "# Unpack probabilities\n",
    "tmp = results_df.rdd.map(lambda x: (x[1], float(x[3][0]), float(x[3][1]),  x[4]) ).toDF(['Label', 'Prob0', 'Prob1', 'prediction'])\n",
    "tmp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add columns of gain and cost\n",
    "tmp2 = tmp.withColumn('Gain', lit(500))\n",
    "tmp2 = tmp2.withColumn('Gain', col('Label')*col('prediction')*col('Gain'))\n",
    "tmp2 = tmp2.withColumn('Cost', lit(5))\n",
    "tmp2.show()\n",
    "tmp2.filter('Label=1 and prediction=1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp4 = sqlContext.sql(\" SELECT row, Label, prediction, Prob0, Prob1, Cost, Gain, sum(Gain) OVER (ORDER BY row) as cumGain FROM tmp3\")\n",
    "tmp4.show(400)\n",
    "tmp4.registerTempTable('tmp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp5 = sqlContext.sql(\" SELECT Label, prediction, Prob0, Prob1, Cost, Gain, cumGain, sum(Cost) OVER (ORDER BY row) as cumCost FROM tmp4\")\n",
    "tmp5.show(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do a left outer join of table 'left_df' with table 'right_df', on 'col_id'\n",
    "# After the join, column 'col_date' is renamed to 'name_date' and converted to Date format \n",
    "# reading the original strings by 'format_src' and creating the dates by 'format_end'\n",
    "def join_date(left_df, right_df, col_id, col_date, name_date, format_src, format_end):\n",
    "    res = left_df.join( \\\n",
    "                        right_df.select(col_id, col_date) \\\n",
    "                                .withColumnRenamed(col_date, name_date), \\\n",
    "                        on=col_id, \\\n",
    "                        how='left_outer' \\\n",
    "                        ) \\\n",
    "                 .withColumn( \\\n",
    "                             colName=name_date, \\\n",
    "                             col=from_unixtime( \\\n",
    "                                                unix_timestamp( \\\n",
    "                                                              col(name_date), \\\n",
    "                                                              format_src \\\n",
    "                                                             ), \\\n",
    "                                                format_end \\\n",
    "                                               ) \\\n",
    "                            )\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_tab = join_date(features_tab, \\\n",
    "                          phone_latest_df.select('ActivityId', 'ActualEnd') \\\n",
    "                                         .withColumnRenamed('ActivityId', 'GreenId'), \\\n",
    "                          'GreenId', \\\n",
    "                          'ActualEnd', \\\n",
    "                          'Green_ActualEnd', \\\n",
    "                          'yyyy-MM-dd hh:mm:ss', \\\n",
    "                          'yyyy-MM-dd hh:mm:ss' \\\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rename_col(df):\n",
    "    oldColumns = df.schema.names\n",
    "    newColumns = [(ii+'_psam') for ii in oldColumns]\n",
    "    df_new = df\n",
    "# exclude last two (careful with this !!!) \n",
    "    for ii in range(0, len(oldColumns)-2):\n",
    "        df_new = df_new.withColumnRenamed(oldColumns[ii], newColumns[ii])\n",
    "    return df_new\n",
    "\n",
    "psam_new = rename_col(psam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rename_columns(df,column_dict):\n",
    "    for key,val in column_dict.items():\n",
    "        df = df.withColumnRenamed(key,val)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Rename the columns\n",
    "column_dict = {\"DMDcampaignName\":\"brand\",\"DMDmailingName\":\"mailing_name\",\\\n",
    "              \"DMDlogDate\":\"log_date\",\"DMDtype\":\"event_type\",\"accountid\":\"accountid\",\\\n",
    "               \"webpower_id\":\"mail_id\"}\n",
    "Ne = rename_columns(Ne,column_dict)\n",
    "Ne = Ne.select(list(column_dict.values()))\n",
    "Ne.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print columns with > 95% nulls\n",
    "#as the other invalid options are possible only in dates column, I add them to the main function\n",
    "\n",
    "bad_columns = []\n",
    "def count_null(df,column,total):\n",
    "    cnt_space = df.where(df[column] == \"\").count()\n",
    "    cnt_NULL = df.where(df[column] == \"NULL\").count()\n",
    "    cnt_null = df.where(df[column].isNull()).count()\n",
    "    cnt_1970 = df.where(df[column] == '0000-00-00 00:00:00').count()\n",
    "    cnt_0000 = df.where(df[column] == '1970-01-01 01:00:00').count()\n",
    "    if((((cnt_null + cnt_NULL + cnt_space+ cnt_1970 + cnt_0000)/total) * 100) > 95):\n",
    "        bad_columns.append('{}'.format(column))\n",
    "\n",
    "#the list of bad columns is automatically saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#When the value is between 30 and 0, then return 1 else 0\n",
    "def smallerThen30(column):\n",
    "    return when(column < 0, 0).otherwise(\\\n",
    "                when(column < 30, 1).otherwise(\\\n",
    "                    when(column > 30,0)))\n",
    "\n",
    "#Now we make for every different pillar, a column if it is in the range of the active period\n",
    "#Datediff in days.\n",
    "list_pillar = [\"uppc\",\"cashback\",\"winaction\"]\n",
    "#Make the activities\n",
    "for pillar in list_pillar:\n",
    "    #Calculate the number of days between the activity and the email sent\n",
    "    activity = activity.withColumn(\"diff\",datediff(col(\"sendout_date\"),col(\"{}_created\".format(pillar))))\n",
    "    #When if the activity is before 30 days of the log date\n",
    "    activity = activity.withColumn(\"{}_activity\".format(pillar),smallerThen30(col(\"diff\")))\n",
    "\n",
    "def isActive(column):\n",
    "    return when(column > 0,1).otherwise(0)\n",
    "\n",
    "activity = activity.withColumn(\"account_isActive\",\\\n",
    "                               isActive(col(\"uppc_activity\")+col(\"cashback_activity\")+col(\"winaction_activity\")))\n",
    "\n",
    "for pillar in list_pillar:\n",
    "    #Clean up\n",
    "    activity = activity.drop(\"{}_created\".format(pillar))\\\n",
    "        .drop(\"{}_activity\".format(pillar))\n",
    "\n",
    "activity = activity.filter(\"account_isActive == '1'\")\n",
    "activity = activity.dropDuplicates()\n",
    "print(\"Number of persons are active before email :{}\".format(activity.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given the ts dates and a dataframe of events, create the aggregated time series with counts of events per week\n",
    "def create_series(dates, events, name):\n",
    "    # Join the given events to the dates. Each event should be joined to exactly one week so the result should has the \n",
    "    # same number of rows as the events table\n",
    "    dates.registerTempTable(\"dates\")\n",
    "    events.registerTempTable(\"events\")\n",
    "    cnt_all = events.count()\n",
    "    tmp = sqlContext.sql(\"SELECT\\\n",
    "                            d.*, e.*\\\n",
    "                          FROM\\\n",
    "                            events e LEFT OUTER JOIN dates d\\\n",
    "                          WHERE\\\n",
    "                            e.sendout_date>=d.week_start AND e.sendout_date<=d.week_end\\\n",
    "                         \")\n",
    "    cnt_res = tmp.count()\n",
    "    if cnt_all==cnt_res:\n",
    "        print('\\tResult has correct number of rows: ' + str(cnt_res))\n",
    "    else:\n",
    "        print('\\tResult has incorrect number of rows: ' + str(cnt_res) + ' expected ' + str(cnt_all))\n",
    "    # Aggregate by time and active/new and count\n",
    "    res = tmp.groupBy('week_start', 'week_end', 'account_isActive', 'account_isNew').count().withColumnRenamed('count', name)\n",
    "    print('\\tAggregation rows: ' + str(res.count()))\n",
    "    return res\n",
    "\n",
    "# Create start table ts dates X isActive X isNew\n",
    "result = sqlContext.sql(\"SELECT\\\n",
    "                            d.*, a.*, n.*\\\n",
    "                         FROM\\\n",
    "                            ts_dates d LEFT OUTER JOIN isActive a LEFT OUTER JOIN isNew n\\\n",
    "                        \")\n",
    "print('Rows: ' + str(result.count()))\n",
    "\n",
    "# Add events series\n",
    "for event in ['sendout', 'click', 'open']:\n",
    "    result = result.join(create_series(ts_dates, wp.filter(\"event_type=='\" + event + \"'\"), event), on=['week_start', 'week_end', 'account_isActive', 'account_isNew'], how='left_outer')\n",
    "    print('Rows: ' + str(result.count()))\n",
    "result = result.join(create_series(ts_dates, wp.filter(\"event_type like '%bounce%'\"), 'bounce'), on=['week_start', 'week_end', 'account_isActive', 'account_isNew'], how='left_outer')\n",
    "print('Rows: ' + str(result.count()))\n",
    "\n",
    "result.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#extract the brand from mailing_name, using regex\n",
    "\n",
    "brands_regex = {'Eurosparen':'^.*(ES|es|Es|[E|e]urosparen).*$',\n",
    "               'Milner':'^.*([M|m]ilner|Miln|MILN).*$',\n",
    "               'Campina':'^.*([C|c]am|AM|[C|c]amp|AMP|[C|c]ampina|CAMPINA).*$',\n",
    "               'Optimel':'^.*([O|o]ptimel|OPT|opt|Opt).*$',\n",
    "               'Mona':'^.*([M|m]ona).*$'}\n",
    "\n",
    "#add a new column called mailing_brand, containing the name of the regex\n",
    "wp_brands = wp.withColumn('mailing_brand', col('mailing_name'))\n",
    "\n",
    "for name, regex in brands_regex.items():\n",
    "    wp_brands = wp_brands.withColumn('mailing_brand', regexp_replace('mailing_brand', str(regex), str(name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create dates array to use for plots (containing only week_start date and not time):\n",
    "\n",
    "weeks = New_Act_pd['week_start'].unique()   #.unique()    #taking each week once\n",
    "dates = []\n",
    "for w in weeks:\n",
    "    dates.append(w[:10]) \n",
    "print ('the number of weeks is: ', len(dates))\n",
    "print (dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove quotation marks\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def NL_crm_sanitize_id(in_id):\n",
    "    if in_id:\n",
    "        # transforms [\"xxx\"] into xxx\n",
    "        if in_id.startswith('\"'):\n",
    "            in_id = in_id[1:]\n",
    "        if in_id.endswith('\"'):\n",
    "            in_id = in_id[:-1]\n",
    "    return in_id\n",
    "\n",
    "f_udf = udf(NL_crm_sanitize_id, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = email_react\n",
    "\n",
    "for col in email_react.columns:\n",
    "#    col_tmp = regexp_replace(col, '\"','')\n",
    "    email_react = email_react.withColumn(col, f_udf(col))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove quates from the column names\n",
    "\n",
    "def rename_col(df):\n",
    "    oldColumns = df.schema.names\n",
    "    newColumns = [ii.strip('\"') for ii in oldColumns]\n",
    "    df_new = df\n",
    "    for ii in range(0, len(oldColumns)):\n",
    "        df_new = df_new.withColumnRenamed(oldColumns[ii], newColumns[ii])\n",
    "    return df_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit,year,weekofyear,col,month,year,concat\n",
    "def get_index(df, columns,sqlContext):\n",
    "    \n",
    "    df.registerTempTable(\"sales\")\n",
    "    string = \"\"\n",
    "    for col in columns:\n",
    "        string += \"s2.{}/s1.{} as ratio{}, s2.{}-s1.{} as index{},\".format(col,col,col,col,col,col)\n",
    "    #Remove the last digit from the sring\n",
    "    string = string[:-1]\n",
    "    \n",
    "    sales_index = sqlContext.sql(\"SELECT s1.province, s1.year as year1, s1.weekN,s2.year as year2,\\\n",
    "                           \"+string+\" FROM sales s1 \\\n",
    "                           JOIN sales s2 ON (s1.province = s2.province) \\\n",
    "                           WHERE s2.year = (s1.year+1) AND s1.weekN = s2.weekN\")\n",
    "    sqlContext.dropTempTable(\"sales\")\n",
    "    return sales_index\n",
    "\n",
    "\n",
    "\n",
    "def count_month_year(df,date_column):\n",
    "    df =  df.withColumn(\"month\",month(date_column)).withColumn(\"year\",year(date_column))\n",
    "    df = df.withColumn(\"month_year\",concat(col(\"month\"),lit(\"_\"),col(\"year\")))\n",
    "    df = df.filter(\"year > 2014 AND year < 2018\")\n",
    "    df.groupby(\"month_year\").count().sort(\"month_year\").show(100,False)\n",
    "    \n",
    "    \n",
    "def date_weekN_year(df,date_column):\n",
    "    df = df.withColumn(\"weekN\",weekofyear(date_column)).withColumn(\"year\",year(date_column))\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible sources of features:\n",
    "=============================\n",
    "\n",
    "- new_PreviousProductUsed2Name\n",
    "- new_CurrentProductUsed2Name\n",
    "- new_CitySourceName\n",
    "- new_LocationName\n",
    "- OwnerId (codes), OwnerIdName( mainly Friso Moments)\n",
    "- new_childrenId (KEY)\n",
    "- statuscode\n",
    "- new_name\n",
    "- new_child_prod_used\n",
    "- new_ChildStage (very good one)\n",
    "- new_EntryAtStage\n",
    "- new_Gender\n",
    "- new_Customer\n",
    "- new_ChildAge (good one)\n",
    "- new_ChildStage (good one)\n",
    "- new_PreviousProductUsed\n",
    "- new_DataSource\n",
    "- new_DataType1\n",
    "- new_DataType2\n",
    "- mew_MethodCollectData\n",
    "- new_CitySource (code)\n",
    "- new_Location\n",
    "- new_CurrentProductUsed2\n",
    "- new_PreviousProductUsed2_PackageSize\n",
    "- new_CurrentProductUsed2\n",
    "- new_PreviousProductUsed2_PackageSize\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------- INIT & LOADING -------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All imports here\n",
    "\n",
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.types import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession already exists\n"
     ]
    }
   ],
   "source": [
    "# Init Spark Session\n",
    "\n",
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    from pyspark.sql import SparkSession\n",
    "    \n",
    "    spark = SparkSession\\\n",
    "                .builder \\\n",
    "                .appName(\"Datascience\") \\\n",
    "                .getOrCreate()\n",
    "    print(\"SparkSession created\")\n",
    "else:\n",
    "    print(\"SparkSession already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- new_CustomerName: string (nullable = true)\n",
      " |-- new_PreviousProductUsed2Name: string (nullable = true)\n",
      " |-- new_CurrentProductUsed2Name: string (nullable = true)\n",
      " |-- new_CitySourceName: string (nullable = true)\n",
      " |-- new_MedicalRepName: string (nullable = true)\n",
      " |-- new_LocationName: string (nullable = true)\n",
      " |-- new_KeyinAgentYomiName: string (nullable = true)\n",
      " |-- CreatedByName: string (nullable = true)\n",
      " |-- CreatedByYomiName: string (nullable = true)\n",
      " |-- ModifiedByName: string (nullable = true)\n",
      " |-- ModifiedByYomiName: string (nullable = true)\n",
      " |-- new_KeyinAgentName: string (nullable = true)\n",
      " |-- OwnerId: string (nullable = true)\n",
      " |-- OwnerIdName: string (nullable = true)\n",
      " |-- OwnerIdYomiName: string (nullable = true)\n",
      " |-- OwnerIdDsc: string (nullable = true)\n",
      " |-- OwningUser: string (nullable = true)\n",
      " |-- new_childrenId: string (nullable = true)\n",
      " |-- CreatedOn: string (nullable = true)\n",
      " |-- CreatedBy: string (nullable = true)\n",
      " |-- ModifiedBy: string (nullable = true)\n",
      " |-- OwningBusinessUnit: string (nullable = true)\n",
      " |-- statecode: string (nullable = true)\n",
      " |-- new_name: string (nullable = true)\n",
      " |-- new_child_prod_used: string (nullable = true)\n",
      " |-- new_ConsumAmount: string (nullable = true)\n",
      " |-- new_ConsumptionPerMonth: string (nullable = true)\n",
      " |-- new_EntryAtStage: string (nullable = true)\n",
      " |-- new_FirstName: string (nullable = true)\n",
      " |-- new_FullName: string (nullable = true)\n",
      " |-- new_Gender: string (nullable = true)\n",
      " |-- new_HasCertification: string (nullable = true)\n",
      " |-- new_LastName: string (nullable = true)\n",
      " |-- new_Customer: string (nullable = true)\n",
      " |-- new_ChildAge: string (nullable = true)\n",
      " |-- new_CurrentChildStage: string (nullable = true)\n",
      " |-- new_HeightAtBirth2: string (nullable = true)\n",
      " |-- new_WeightAtBirth2: string (nullable = true)\n",
      " |-- new_MigratedId: string (nullable = true)\n",
      " |-- new_NumberOfTin: string (nullable = true)\n",
      " |-- new_DataType1: string (nullable = true)\n",
      " |-- new_Sampling: string (nullable = true)\n",
      " |-- new_UsedFrisoDLfor3recentmonths: string (nullable = true)\n",
      " |-- new_CitySource: string (nullable = true)\n",
      " |-- new_Location: string (nullable = true)\n",
      " |-- new_MedicalRep: string (nullable = true)\n",
      " |-- new_CurrentProductUsed2: string (nullable = true)\n",
      " |-- new_PreviousProductUsed2: string (nullable = true)\n",
      " |-- new_KeyinAgent: string (nullable = true)\n",
      " |-- new_CurrentProductUsed2_PakageSize: string (nullable = true)\n",
      " |-- new_PreviousProductUsed2_PakageSize: string (nullable = true)\n",
      " |-- new_AgeByMonth: string (nullable = true)\n",
      " |-- new_AgeByDay: string (nullable = true)\n",
      " |-- new_CurrentProductUsedModified: string (nullable = true)\n",
      " |-- ModifiedOn: string (nullable = true)\n",
      " |-- new_DateOfBirth: string (nullable = true)\n",
      " |-- new_DataofMonth: string (nullable = true)\n",
      " |-- new_DOBModified: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load clean phonecall data\n",
    "children_df = spark.read.parquet(\"maprfs:///test_zone/ds_playground/projects/marketing_conversion/1_cleaning/data/children_new\")\n",
    "children_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List of interesting features\n",
    "\n",
    "ft_list = ['new_CurrentProductUsed2Name', 'new_CitySourceName', \n",
    " 'OwnerIdName','new_childrenId', 'statecode', 'new_CurrentChildStage', 'new_EntryAtStage',\n",
    "'new_ChildAge', 'new_DataType1', \n",
    "'new_CitySource', 'new_PreviousProductUsed2_PakageSize', 'new_CurrentProductUsed2',\n",
    "'new_PreviousProductUsed2_PakageSize']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2830262"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "children_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_counts_group(df, column):\n",
    "    return df.groupBy(column).count().sort(\"count\", ascending = False).show(10,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+-------+\n",
      "|new_CurrentProductUsed2Name        |count  |\n",
      "+-----------------------------------+-------+\n",
      "|                                   |1082569|\n",
      "|Sữa Mẹ                             |202254 |\n",
      "|Fresh milk                         |135741 |\n",
      "|-                                  |122322 |\n",
      "|Friso Gold 3 12x900gr              |103949 |\n",
      "|Friso Gold 1 24x400gr              |96469  |\n",
      "|Friso Gold 3 24x400gr              |82585  |\n",
      "|Others                             |79650  |\n",
      "|Non Use                            |72471  |\n",
      "|IMP FRISOLAC GOLD 1 24X400G SUNRISE|62805  |\n",
      "+-----------------------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+------------------+-------+\n",
      "|new_CitySourceName|count  |\n",
      "+------------------+-------+\n",
      "|                  |1071694|\n",
      "|HCM               |559249 |\n",
      "|Hà Nội            |303417 |\n",
      "|North (Others)    |237564 |\n",
      "|Central (Others)  |135900 |\n",
      "|East (Others)     |131250 |\n",
      "|Mekong (Others)   |113551 |\n",
      "|Biên Hòa          |68481  |\n",
      "|Đà Nẵng           |56949  |\n",
      "|Cần Thơ           |52907  |\n",
      "+------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------+-------+\n",
      "|OwnerIdName         |count  |\n",
      "+--------------------+-------+\n",
      "|Friso Moments       |1208120|\n",
      "|Vo Thi Cam Van      |712834 |\n",
      "|Tran Thi Nha Truc   |22257  |\n",
      "|SAI CRM             |20486  |\n",
      "|Dang Ngoc Lam Thanh |19774  |\n",
      "|Friso RD            |19689  |\n",
      "|Friso NU            |16361  |\n",
      "|Tran Thi Diem Ly    |16277  |\n",
      "|Doan Bich Linh      |13637  |\n",
      "|Nguyen Thi Ngoc Linh|13002  |\n",
      "+--------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+------------------------------------+-----+\n",
      "|new_childrenId                      |count|\n",
      "+------------------------------------+-----+\n",
      "|1EBB7F43-BC9A-469F-9D66-5043A148F313|1    |\n",
      "|70CD0D17-81FF-4B7A-8616-CEAFCB583406|1    |\n",
      "|60A401F1-188C-E511-8983-AC162D73558B|1    |\n",
      "|A98946CE-C12D-E411-A572-AC162D72FE2C|1    |\n",
      "|2F35E7E9-D5CB-4D61-813E-D91A781D6A1D|1    |\n",
      "|EB878C0E-7E4F-4B8B-8570-7A40385CF2ED|1    |\n",
      "|08A1781E-1066-4868-BF03-89BBC84B32E5|1    |\n",
      "|484B20B6-CEAF-E611-B179-AC162D72E517|1    |\n",
      "|503ED35D-A1E2-4212-873A-18D10DD02229|1    |\n",
      "|23370A53-F71E-E511-9F9A-AC162D73558B|1    |\n",
      "+------------------------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+---------+-------+\n",
      "|statecode|count  |\n",
      "+---------+-------+\n",
      "|0        |2808063|\n",
      "|1        |22199  |\n",
      "+---------+-------+\n",
      "\n",
      "+--------------------------------------+-------+\n",
      "|new_CurrentChildStage                 |count  |\n",
      "+--------------------------------------+-------+\n",
      "|Stage 4 (25 - 48)                     |1154067|\n",
      "|Stage 5 (49 - )                       |540734 |\n",
      "|Stage 3 (13 - 24)                     |509245 |\n",
      "|Stage 2 (7 - 12)                      |315706 |\n",
      "|Stage 1 (0 - 6)                       |280596 |\n",
      "|                                      |17020  |\n",
      "|Stage 0 (Pregnancy)                   |12763  |\n",
      "|null                                  |124    |\n",
      "|{6088E99D-334A-E411-A713-AC162D73558B}|1      |\n",
      "|{10B4234D-A323-4737-8101-07B239B3A2AF}|1      |\n",
      "+--------------------------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------------------+-------+\n",
      "|new_EntryAtStage   |count  |\n",
      "+-------------------+-------+\n",
      "|1                  |1127392|\n",
      "|                   |1001863|\n",
      "|3                  |400210 |\n",
      "|4                  |163332 |\n",
      "|2                  |114967 |\n",
      "|0                  |22412  |\n",
      "|null               |79     |\n",
      "|03/03/2015 17:00:00|1      |\n",
      "|05/27/2016 17:00:00|1      |\n",
      "|04/17/2014 17:00:00|1      |\n",
      "+-------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+------------------------+-----+\n",
      "|new_ChildAge            |count|\n",
      "+------------------------+-----+\n",
      "|                        |17664|\n",
      "|3 years 30 days         |15461|\n",
      "|0 day                   |12763|\n",
      "|5 years 30 days         |10318|\n",
      "|2 years 30 days         |9283 |\n",
      "|1 year 1 month          |8226 |\n",
      "|4 years 30 days         |7498 |\n",
      "|3 years 1 month 21 days |7457 |\n",
      "|3 years 2 months 11 days|5157 |\n",
      "|4 months 1 day          |4798 |\n",
      "+------------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------------+-------+\n",
      "|new_DataType1|count  |\n",
      "+-------------+-------+\n",
      "|             |1290134|\n",
      "|1            |1278515|\n",
      "|2            |261489 |\n",
      "|null         |124    |\n",
      "+-------------+-------+\n",
      "\n",
      "+------------------------------------+-------+\n",
      "|new_CitySource                      |count  |\n",
      "+------------------------------------+-------+\n",
      "|                                    |1071698|\n",
      "|1B43E369-8EEE-E211-94C7-AC162D72E517|559124 |\n",
      "|83127355-8EEE-E211-94C7-AC162D72E517|303415 |\n",
      "|67CD354F-8EEE-E211-94C7-AC162D72E517|237564 |\n",
      "|C360934A-91EE-E211-94C7-AC162D72E517|135900 |\n",
      "|9B84717B-8EEE-E211-94C7-AC162D72E517|131248 |\n",
      "|87BE7682-8EEE-E211-94C7-AC162D72E517|113550 |\n",
      "|F7E02474-8EEE-E211-94C7-AC162D72E517|68481  |\n",
      "|85127355-8EEE-E211-94C7-AC162D72E517|56949  |\n",
      "|9D84717B-8EEE-E211-94C7-AC162D72E517|52907  |\n",
      "+------------------------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----------------------------------+-------+\n",
      "|new_PreviousProductUsed2_PakageSize|count  |\n",
      "+-----------------------------------+-------+\n",
      "|                                   |1445687|\n",
      "|100000022                          |1087462|\n",
      "|100000016                          |197212 |\n",
      "|100000019                          |88270  |\n",
      "|100000004                          |8616   |\n",
      "|100000020                          |1451   |\n",
      "|100000007                          |721    |\n",
      "|100000002                          |268    |\n",
      "|null                               |124    |\n",
      "|100000006                          |108    |\n",
      "+-----------------------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+------------------------------------+-------+\n",
      "|new_CurrentProductUsed2             |count  |\n",
      "+------------------------------------+-------+\n",
      "|                                    |1082564|\n",
      "|5C8C81EC-B5E0-49DD-8593-F6F5D1FD0A19|202245 |\n",
      "|F6D7C9DA-2CED-E211-94C7-AC162D72E517|135740 |\n",
      "|D64B0DB8-5BFA-E211-94C7-AC162D72E517|118799 |\n",
      "|2E363175-57C8-E211-97C4-AC162D72E517|103949 |\n",
      "|24363175-57C8-E211-97C4-AC162D72E517|96440  |\n",
      "|2C363175-57C8-E211-97C4-AC162D72E517|82584  |\n",
      "|FAD7C9DA-2CED-E211-94C7-AC162D72E517|79649  |\n",
      "|F8D7C9DA-2CED-E211-94C7-AC162D72E517|72471  |\n",
      "|0418671E-A9F4-E511-A7BE-AC162D73558B|62805  |\n",
      "+------------------------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----------------------------------+-------+\n",
      "|new_PreviousProductUsed2_PakageSize|count  |\n",
      "+-----------------------------------+-------+\n",
      "|                                   |1445687|\n",
      "|100000022                          |1087462|\n",
      "|100000016                          |197212 |\n",
      "|100000019                          |88270  |\n",
      "|100000004                          |8616   |\n",
      "|100000020                          |1451   |\n",
      "|100000007                          |721    |\n",
      "|100000002                          |268    |\n",
      "|null                               |124    |\n",
      "|100000006                          |108    |\n",
      "+-----------------------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col in ft_list:\n",
    "    get_counts_group(children_df,col)\n",
    "\n",
    "#children_df.groupBy('new_PreviousProductUsed2_PakageSize').count().sort(desc('count')).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Comments:\n",
    "- statcode is mainly 0\n",
    "- new_CurrentChild Stage and new_ChildAge refer to the same\n",
    "- new_EntryAtStage\n",
    "- new_DataType1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---- FEATURES ------\n",
    "\n",
    "- 1. new_ChildAge and new_CurrentChildStage are the same\n",
    "- 2. new_DataType1\n",
    "- 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature: child_stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = children_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------------------------+\n",
      "|new_CurrentChildStage|new_ChildAge             |\n",
      "+---------------------+-------------------------+\n",
      "|Stage 3 (13 - 24)    |1 year 5 months 8 days   |\n",
      "|Stage 4 (25 - 48)    |4 years 10 days          |\n",
      "|Stage 4 (25 - 48)    |3 years 30 days          |\n",
      "|Stage 4 (25 - 48)    |2 years 5 months 5 days  |\n",
      "|Stage 4 (25 - 48)    |3 years 4 months 28 days |\n",
      "|Stage 3 (13 - 24)    |1 year 4 months 21 days  |\n",
      "|Stage 4 (25 - 48)    |4 years 29 days          |\n",
      "|Stage 3 (13 - 24)    |1 year 4 months 22 days  |\n",
      "|Stage 4 (25 - 48)    |2 years 1 month 6 days   |\n",
      "|Stage 3 (13 - 24)    |1 year 2 months 6 days   |\n",
      "|Stage 4 (25 - 48)    |2 years 3 months 16 days |\n",
      "|Stage 4 (25 - 48)    |2 years 2 months 26 days |\n",
      "|Stage 2 (7 - 12)     |9 months 9 days          |\n",
      "|Stage 4 (25 - 48)    |3 years 4 months 15 days |\n",
      "|Stage 4 (25 - 48)    |2 years 9 months 8 days  |\n",
      "|Stage 4 (25 - 48)    |3 years 7 months 8 days  |\n",
      "|Stage 4 (25 - 48)    |3 years 3 months 28 days |\n",
      "|Stage 4 (25 - 48)    |2 years 3 months 15 days |\n",
      "|Stage 4 (25 - 48)    |2 years 6 months 29 days |\n",
      "|Stage 5 (49 - )      |4 years 4 months 21 days |\n",
      "|Stage 2 (7 - 12)     |7 months 16 days         |\n",
      "|Stage 4 (25 - 48)    |2 years 8 months 22 days |\n",
      "|Stage 4 (25 - 48)    |2 years 5 months         |\n",
      "|Stage 4 (25 - 48)    |3 years 10 months 1 day  |\n",
      "|Stage 4 (25 - 48)    |3 years 2 months         |\n",
      "|Stage 2 (7 - 12)     |8 months 27 days         |\n",
      "|Stage 3 (13 - 24)    |1 year 9 months 5 days   |\n",
      "|Stage 0 (Pregnancy)  |0 day                    |\n",
      "|Stage 4 (25 - 48)    |3 years 8 months 16 days |\n",
      "|Stage 3 (13 - 24)    |1 year 1 month 8 days    |\n",
      "|Stage 4 (25 - 48)    |3 years 3 months 12 days |\n",
      "|Stage 4 (25 - 48)    |3 years 3 months 6 days  |\n",
      "|Stage 1 (0 - 6)      |27 days                  |\n",
      "|Stage 3 (13 - 24)    |1 year 6 months 24 days  |\n",
      "|Stage 4 (25 - 48)    |2 years 5 months 2 days  |\n",
      "|Stage 5 (49 - )      |5 years 30 days          |\n",
      "|Stage 4 (25 - 48)    |2 years 5 months 3 days  |\n",
      "|Stage 4 (25 - 48)    |2 years 5 months         |\n",
      "|Stage 4 (25 - 48)    |2 years 5 months 20 days |\n",
      "|Stage 4 (25 - 48)    |2 years 10 months 12 days|\n",
      "|Stage 4 (25 - 48)    |3 years 4 months 4 days  |\n",
      "|Stage 1 (0 - 6)      |16 days                  |\n",
      "|Stage 4 (25 - 48)    |2 years 9 months 10 days |\n",
      "|Stage 1 (0 - 6)      |10 days                  |\n",
      "|Stage 5 (49 - )      |9 years 19 days          |\n",
      "|Stage 4 (25 - 48)    |3 years 2 months 10 days |\n",
      "|Stage 4 (25 - 48)    |3 years 9 months 23 days |\n",
      "|Stage 3 (13 - 24)    |1 year 3 months 28 days  |\n",
      "|Stage 1 (0 - 6)      |1 month 19 days          |\n",
      "|Stage 4 (25 - 48)    |2 years 4 months 17 days |\n",
      "+---------------------+-------------------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('new_CurrentChildStage', 'new_ChildAge').show(50, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.withColumn('child_stage', df['new_CurrentChildStage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------------+\n",
      "|child_stage      |new_CurrentChildStage|\n",
      "+-----------------+---------------------+\n",
      "|Stage 3 (13 - 24)|Stage 3 (13 - 24)    |\n",
      "|Stage 4 (25 - 48)|Stage 4 (25 - 48)    |\n",
      "|Stage 4 (25 - 48)|Stage 4 (25 - 48)    |\n",
      "|Stage 4 (25 - 48)|Stage 4 (25 - 48)    |\n",
      "|Stage 4 (25 - 48)|Stage 4 (25 - 48)    |\n",
      "|Stage 3 (13 - 24)|Stage 3 (13 - 24)    |\n",
      "|Stage 4 (25 - 48)|Stage 4 (25 - 48)    |\n",
      "|Stage 3 (13 - 24)|Stage 3 (13 - 24)    |\n",
      "|Stage 4 (25 - 48)|Stage 4 (25 - 48)    |\n",
      "|Stage 3 (13 - 24)|Stage 3 (13 - 24)    |\n",
      "|Stage 4 (25 - 48)|Stage 4 (25 - 48)    |\n",
      "|Stage 4 (25 - 48)|Stage 4 (25 - 48)    |\n",
      "|Stage 2 (7 - 12) |Stage 2 (7 - 12)     |\n",
      "|Stage 4 (25 - 48)|Stage 4 (25 - 48)    |\n",
      "|Stage 4 (25 - 48)|Stage 4 (25 - 48)    |\n",
      "|Stage 4 (25 - 48)|Stage 4 (25 - 48)    |\n",
      "|Stage 4 (25 - 48)|Stage 4 (25 - 48)    |\n",
      "|Stage 4 (25 - 48)|Stage 4 (25 - 48)    |\n",
      "|Stage 4 (25 - 48)|Stage 4 (25 - 48)    |\n",
      "|Stage 5 (49 - )  |Stage 5 (49 - )      |\n",
      "+-----------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('child_stage', 'new_CurrentChildStage').show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def child_stage(column):\n",
    "    return (when(column == 'Stage 0 (Pregnancy)', 'Stage 0 (Pregnancy)').\\\n",
    "    when(column == 'Stage 1 (0 - 6)', 'Stage 1 (0 - 6)').\\\n",
    "    when(column == 'Stage 2 (7 - 12)', 'Stage 2 (7 - 12)').\\\n",
    "    when(column == 'Stage 3 (13 - 24)', 'Stage 3 (13 - 24)').\\\n",
    "    when(column == 'Stage 4 (25 - 48)', 'Stage 4 (25 - 48)').\\\n",
    "    when(column == 'Stage 5 (49 - )', 'Stage 5 (49 - ) ').\\\n",
    "    otherwise('other'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.withColumn('child_stage', child_stage(col('new_CurrentChildStage')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+\n",
      "|child_stage        |count  |\n",
      "+-------------------+-------+\n",
      "|Stage 4 (25 - 48)  |1154067|\n",
      "|Stage 5 (49 - )    |540734 |\n",
      "|Stage 3 (13 - 24)  |509245 |\n",
      "|Stage 2 (7 - 12)   |315706 |\n",
      "|Stage 1 (0 - 6)    |280596 |\n",
      "|other              |17151  |\n",
      "|Stage 0 (Pregnancy)|12763  |\n",
      "+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('child_stage').count().sort(desc('count')).show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature: data_type\n",
    "- don't know the meaning of this one but potentiall might be useful (e.g. if data is coming from hospital or some other source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+\n",
      "|new_DataType1|count  |\n",
      "+-------------+-------+\n",
      "|             |1290134|\n",
      "|1            |1278515|\n",
      "|2            |261489 |\n",
      "|null         |124    |\n",
      "+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('new_DataType1').count().sort(desc('count')).show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace(column):\n",
    "    return (when(column == '1','1' ).when(column=='2', '2').otherwise('blank'))\n",
    "\n",
    "df = df.withColumn('data_type', replace(col('new_DataType1')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|data_type|count  |\n",
      "+---------+-------+\n",
      "|blank    |1290258|\n",
      "|1        |1278515|\n",
      "|2        |261489 |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('data_type').count().sort(desc('count')).show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature: entry_at_stage\n",
    "- don't know the meaning of this one but potentiall might be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+\n",
      "|new_EntryAtStage   |count  |\n",
      "+-------------------+-------+\n",
      "|1                  |1127392|\n",
      "|                   |1001863|\n",
      "|3                  |400210 |\n",
      "|4                  |163332 |\n",
      "|2                  |114967 |\n",
      "|0                  |22412  |\n",
      "|null               |79     |\n",
      "|05/27/2016 17:00:00|1      |\n",
      "|10/05/2013 17:00:00|1      |\n",
      "|04/18/2016 17:00:00|1      |\n",
      "|03/03/2015 17:00:00|1      |\n",
      "|06/24/2013 17:00:00|1      |\n",
      "|04/17/2014 17:00:00|1      |\n",
      "|04/11/2015 17:00:00|1      |\n",
      "+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('new_EntryAtStage').count().sort(desc('count')).show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replaceE(column):\n",
    "    return (when(column == '0', '0').when(column == '1','1' ).when(column=='2', '2').\\\n",
    "            when(column == '3', '3').when(column=='4','4').\\\n",
    "            otherwise('mess'))\n",
    "\n",
    "df = df.withColumn('entry_at_stage', replaceE(col('new_EntryAtStage')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+\n",
      "|entry_at_stage|count  |\n",
      "+--------------+-------+\n",
      "|1             |1127392|\n",
      "|mess          |1001949|\n",
      "|3             |400210 |\n",
      "|4             |163332 |\n",
      "|2             |114967 |\n",
      "|0             |22412  |\n",
      "+--------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('entry_at_stage').count().sort(desc('count')).show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature: current_product_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+-------+\n",
      "|new_CurrentProductUsed2Name           |count  |\n",
      "+--------------------------------------+-------+\n",
      "|                                      |1082569|\n",
      "|Sữa Mẹ                                |202254 |\n",
      "|Fresh milk                            |135741 |\n",
      "|-                                     |122322 |\n",
      "|Friso Gold 3 12x900gr                 |103949 |\n",
      "|Friso Gold 1 24x400gr                 |96469  |\n",
      "|Friso Gold 3 24x400gr                 |82585  |\n",
      "|Others                                |79650  |\n",
      "|Non Use                               |72471  |\n",
      "|IMP FRISOLAC GOLD 1 24X400G SUNRISE   |62805  |\n",
      "|IMP FRISO GOLD 4 24X400G SUNRISE      |36634  |\n",
      "|Grow Plus+ (>2y) (suy DD)             |33835  |\n",
      "|IMP FRISO GOLD 4 12X900G SUNRISE      |30077  |\n",
      "|Friso Gold 4 12x900gr                 |29910  |\n",
      "|Friso Gold 2 24x400gr                 |27953  |\n",
      "|Friso Gold 2 12x900gr                 |27808  |\n",
      "|Enfagrow A+ (1y-3y)                   |26723  |\n",
      "|Pediasure                             |26187  |\n",
      "|Grow 3 (1y-3y)                        |25870  |\n",
      "|Dielac Alpha Gold 3                   |21958  |\n",
      "|Friso Gold 1 12x900gr                 |19241  |\n",
      "|Friso Gold 3 6x1500gr                 |18982  |\n",
      "|Dielac Alpha 1                        |18818  |\n",
      "|Dielac Optimum 3 (dễ tiêu hóa)        |18457  |\n",
      "|Nuti IQ 3                             |15853  |\n",
      "|Dutch Lady Khám Phá Gold 2-4 (12x900g)|14953  |\n",
      "|Dielac Alpha 123                      |14106  |\n",
      "|Enfalac A+ (0-6m)                     |13894  |\n",
      "|Friso 3 12x900gr                      |11706  |\n",
      "|Similac  IQ (0m-6m)                   |10403  |\n",
      "|Friso 1 12x900gr                      |9932   |\n",
      "|Friso Gold Pedia lon thiec 900g       |9751   |\n",
      "|Dielac Alpha Gold 123                 |9519   |\n",
      "|Similac Total Comfort 3               |9147   |\n",
      "|DUTCH LADY 2-4 KHÁM PHÁ lon thiếc 900g|8340   |\n",
      "|IMP FRISOLAC GOLD 3 12X900G SUNRISE   |7919   |\n",
      "|Dielac Alpha 2                        |7711   |\n",
      "|Friso Gold Pedia lon thiec 400g       |7453   |\n",
      "|Gain Plus IQ (1y-3Y)                  |7380   |\n",
      "|Friso 4 12x900gr                      |6698   |\n",
      "|IMP FRISOLAC GOLD 3 24X400G SUNRISE   |6584   |\n",
      "|Friso 2 12x900gr                      |5882   |\n",
      "|Nan 1                                 |5504   |\n",
      "|IMP FRISO GOLD 4 6x1500G SUNRISE      |5330   |\n",
      "|IMP FRISOLAC GOLD 2 12X900G SUNRISE   |5318   |\n",
      "|Friso 3 6x1500gr                      |5275   |\n",
      "|Dielac Optimum 2 (dễ tiêu hóa)        |5042   |\n",
      "|Nan Pro 3                             |4893   |\n",
      "|Grow 4 (3y-6y)                        |4782   |\n",
      "|Friso Gold 4 vani 6x1500g             |4681   |\n",
      "+--------------------------------------+-------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('new_CurrentProductUsed2Name').count().sort(desc('count')).show(50,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Important .* means any string before or after (e.g. '.*xxx.*' matches sthgxxxsthgxxx)\n",
    "# \"^\\s*$\" is empty string\n",
    "\n",
    "\n",
    "df = df.withColumn('current_product_used', regexp_replace('new_CurrentProductUsed2Name','.*Friso Gold 1.*|.*Friso 1.*|.*GOLD 1.*', 'Friso Gold 1'))\n",
    "df = df.withColumn('current_product_used', regexp_replace('current_product_used','.*Friso Gold 2.*|.*GOLD 2.*', 'Friso Gold 2'))\n",
    "df = df.withColumn('current_product_used', regexp_replace('current_product_used','.*Friso Gold 3.*|.*Friso 3.*|.*GOLD 3.*', 'Friso Gold 3'))\n",
    "df = df.withColumn('current_product_used', regexp_replace('current_product_used','.*Friso Gold 4.*|.*GOLD 4.*', 'Friso Gold 4'))\n",
    "df = df.withColumn('current_product_used', regexp_replace('current_product_used','.*DUTCH LADY.*|.*Dutch Lady.*', 'Dutch Lady'))\n",
    "#df = df.withColumn('current_product_used', regexp_replace('current_product_used','.*Dielac Optimum.*', 'Dielac Optimum'))\n",
    "df = df.withColumn('current_product_used', regexp_replace('current_product_used','.*Dielac.*', 'Dielac'))\n",
    "df = df.withColumn('current_product_used', regexp_replace('current_product_used','.*Similac.*', 'Similac'))\n",
    "df = df.withColumn('current_product_used', regexp_replace('current_product_used','.*Friso Gold Pedia.*', 'Friso Gold Pedia'))\n",
    "df = df.withColumn('current_product_used', regexp_replace('current_product_used','.*Nuti.*', 'Nuti'))\n",
    "df = df.withColumn('current_product_used', regexp_replace('current_product_used','.*Nan.*', 'Nan'))\n",
    "df = df.withColumn('current_product_used', regexp_replace('current_product_used','.*Dutch Baby.*|.*Dutchbaby.*', 'Dutch Baby'))\n",
    "df = df.withColumn('current_product_used', regexp_replace('current_product_used','.*Grow.*', 'Grow'))\n",
    "df = df.withColumn('current_product_used', regexp_replace('current_product_used','.*Gain.*', 'Gain'))\n",
    "df = df.withColumn('current_product_used', regexp_replace('current_product_used','.*Dumex.*', 'Dumex'))\n",
    "df = df.withColumn('current_product_used', regexp_replace('current_product_used',\"^\\s*$\", 'Blank'))\n",
    "df = df.withColumn('current_product_used', regexp_replace('current_product_used',\"^-\", 'Blank'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+-------+\n",
      "|current_product_used                 |count  |\n",
      "+-------------------------------------+-------+\n",
      "|Blank                                |1204891|\n",
      "|Friso Gold 3                         |243927 |\n",
      "|Sữa Mẹ                               |202254 |\n",
      "|Friso Gold 1                         |194544 |\n",
      "|Fresh milk                           |135741 |\n",
      "|Dielac                               |119788 |\n",
      "|Friso Gold 4                         |107177 |\n",
      "|Others                               |79650  |\n",
      "|Non Use                              |72471  |\n",
      "|Grow                                 |68959  |\n",
      "|Friso Gold 2                         |64690  |\n",
      "|Dutch Lady                           |45140  |\n",
      "|Similac                              |38969  |\n",
      "|Nuti                                 |26783  |\n",
      "|Enfagrow A+ (1y-3y)                  |26723  |\n",
      "|Pediasure                            |26187  |\n",
      "|Nan                                  |21160  |\n",
      "|Dutch Baby                           |17543  |\n",
      "|Friso Gold Pedia                     |17204  |\n",
      "|Enfalac A+ (0-6m)                    |13894  |\n",
      "|Dumex                                |9044   |\n",
      "|Gain                                 |8950   |\n",
      "|Friso 4 12x900gr                     |6698   |\n",
      "|Friso 2 12x900gr                     |5882   |\n",
      "|Enfapro A+ (6-12m)                   |3999   |\n",
      "|IMP FRISO GOLD 5 12X900G SUNRISE     |3769   |\n",
      "|Meiji gold 3                         |3474   |\n",
      "|Anmum                                |3464   |\n",
      "|Enfakid A+                           |2624   |\n",
      "|XO 3                                 |2532   |\n",
      "|Enfamama                             |2500   |\n",
      "|Physiolac 3                          |2242   |\n",
      "|Meiji 3                              |1994   |\n",
      "|PediaPlus (biếng ăn) (1y-10y)        |1962   |\n",
      "|DL Step 1(not use)                   |1752   |\n",
      "|Meiji 1                              |1601   |\n",
      "|CGHL 123 Gold 12x900gr               |1567   |\n",
      "|Aptamil 3                            |1556   |\n",
      "|Enfagrow A+ (1y-3y) (not use)        |1332   |\n",
      "|IMP FRISO GOLD 5 6x1500G SUNRISE     |1319   |\n",
      "|XO 1                                 |1282   |\n",
      "|Enfapro A+ (6-12m)(not use)          |1277   |\n",
      "|S26 Gold 3(1m-3m)                    |1253   |\n",
      "|UHT 20+ 180ml có đường               |1186   |\n",
      "|Frisomum (not use)                   |1105   |\n",
      "|IMP DL 123 900gr                     |1029   |\n",
      "|Physiolac 1                          |999    |\n",
      "|IMP DL 123 gold 400gr                |953    |\n",
      "|Frisolac Premature 24x400gr          |935    |\n",
      "|DUTCH BABY 6-12 TẬP ĐI lon thiếc 900g|901    |\n",
      "+-------------------------------------+-------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('current_product_used').count().sort(desc('count')).show(50,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment: this feature cannot be used directly inmodelling but should be useful anyway !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Feature: city_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+\n",
      "|new_CitySourceName|count  |\n",
      "+------------------+-------+\n",
      "|                  |1071694|\n",
      "|HCM               |559249 |\n",
      "|Hà Nội            |303417 |\n",
      "|North (Others)    |237564 |\n",
      "|Central (Others)  |135900 |\n",
      "|East (Others)     |131250 |\n",
      "|Mekong (Others)   |113551 |\n",
      "|Biên Hòa          |68481  |\n",
      "|Đà Nẵng           |56949  |\n",
      "|Cần Thơ           |52907  |\n",
      "|Hải Phòng         |43871  |\n",
      "|-                 |34512  |\n",
      "|Nha Trang         |20917  |\n",
      "+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('new_CitySourceName').count().sort(desc('count')).show(50,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.withColumn('city_name', regexp_replace('new_CitySourceName', \"^\\s*$\", \"Blank\"))\n",
    "df = df.withColumn('city_name', regexp_replace('city_name', \"^-\", \"Blank\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+\n",
      "|city_name       |count  |\n",
      "+----------------+-------+\n",
      "|Blank           |1106206|\n",
      "|HCM             |559249 |\n",
      "|Hà Nội          |303417 |\n",
      "|North (Others)  |237564 |\n",
      "|Central (Others)|135900 |\n",
      "|East (Others)   |131250 |\n",
      "|Mekong (Others) |113551 |\n",
      "|Biên Hòa        |68481  |\n",
      "|Đà Nẵng         |56949  |\n",
      "|Cần Thơ         |52907  |\n",
      "|Hải Phòng       |43871  |\n",
      "|Nha Trang       |20917  |\n",
      "+----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('city_name').count().sort(desc('count')).show(50,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- new_CustomerName: string (nullable = true)\n",
      " |-- new_PreviousProductUsed2Name: string (nullable = true)\n",
      " |-- new_CurrentProductUsed2Name: string (nullable = true)\n",
      " |-- new_CitySourceName: string (nullable = true)\n",
      " |-- new_MedicalRepName: string (nullable = true)\n",
      " |-- new_LocationName: string (nullable = true)\n",
      " |-- new_KeyinAgentYomiName: string (nullable = true)\n",
      " |-- CreatedByName: string (nullable = true)\n",
      " |-- CreatedByYomiName: string (nullable = true)\n",
      " |-- ModifiedByName: string (nullable = true)\n",
      " |-- ModifiedByYomiName: string (nullable = true)\n",
      " |-- new_KeyinAgentName: string (nullable = true)\n",
      " |-- OwnerId: string (nullable = true)\n",
      " |-- OwnerIdName: string (nullable = true)\n",
      " |-- OwnerIdYomiName: string (nullable = true)\n",
      " |-- OwnerIdDsc: string (nullable = true)\n",
      " |-- OwningUser: string (nullable = true)\n",
      " |-- new_childrenId: string (nullable = true)\n",
      " |-- CreatedOn: string (nullable = true)\n",
      " |-- CreatedBy: string (nullable = true)\n",
      " |-- ModifiedBy: string (nullable = true)\n",
      " |-- OwningBusinessUnit: string (nullable = true)\n",
      " |-- statecode: string (nullable = true)\n",
      " |-- new_name: string (nullable = true)\n",
      " |-- new_child_prod_used: string (nullable = true)\n",
      " |-- new_ConsumAmount: string (nullable = true)\n",
      " |-- new_ConsumptionPerMonth: string (nullable = true)\n",
      " |-- new_EntryAtStage: string (nullable = true)\n",
      " |-- new_FirstName: string (nullable = true)\n",
      " |-- new_FullName: string (nullable = true)\n",
      " |-- new_Gender: string (nullable = true)\n",
      " |-- new_HasCertification: string (nullable = true)\n",
      " |-- new_LastName: string (nullable = true)\n",
      " |-- new_Customer: string (nullable = true)\n",
      " |-- new_ChildAge: string (nullable = true)\n",
      " |-- new_CurrentChildStage: string (nullable = true)\n",
      " |-- new_HeightAtBirth2: string (nullable = true)\n",
      " |-- new_WeightAtBirth2: string (nullable = true)\n",
      " |-- new_MigratedId: string (nullable = true)\n",
      " |-- new_NumberOfTin: string (nullable = true)\n",
      " |-- new_DataType1: string (nullable = true)\n",
      " |-- new_Sampling: string (nullable = true)\n",
      " |-- new_UsedFrisoDLfor3recentmonths: string (nullable = true)\n",
      " |-- new_CitySource: string (nullable = true)\n",
      " |-- new_Location: string (nullable = true)\n",
      " |-- new_MedicalRep: string (nullable = true)\n",
      " |-- new_CurrentProductUsed2: string (nullable = true)\n",
      " |-- new_PreviousProductUsed2: string (nullable = true)\n",
      " |-- new_KeyinAgent: string (nullable = true)\n",
      " |-- new_CurrentProductUsed2_PakageSize: string (nullable = true)\n",
      " |-- new_PreviousProductUsed2_PakageSize: string (nullable = true)\n",
      " |-- new_AgeByMonth: string (nullable = true)\n",
      " |-- new_AgeByDay: string (nullable = true)\n",
      " |-- new_CurrentProductUsedModified: string (nullable = true)\n",
      " |-- ModifiedOn: string (nullable = true)\n",
      " |-- new_DateOfBirth: string (nullable = true)\n",
      " |-- new_DataofMonth: string (nullable = true)\n",
      " |-- new_DOBModified: string (nullable = true)\n",
      " |-- child_stage: string (nullable = false)\n",
      " |-- data_type: string (nullable = false)\n",
      " |-- entry_at_stage: string (nullable = false)\n",
      " |-- current_product_used: string (nullable = true)\n",
      " |-- city_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "children_ft = df.select(\"new_childrenId\", \"child_stage\", \"data_type\",\"entry_at_stage\",\"current_product_used\",\"city_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------- SAVING -------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved successfully!\n"
     ]
    }
   ],
   "source": [
    "children_ft.write.parquet(\"maprfs:///test_zone/ds_playground/projects/marketing_conversion/2_features/data/children_features\", mode=\"overwrite\")\n",
    "print('Saved successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
